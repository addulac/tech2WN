\section{Experimental validation}
\label{sec:exps}

%
% Corpus
%


We experimented our models and inference on several real world networks,  directed or undirected. Theirs statistics and properties are summarized in table \ref{table:corpus} and detailed descriptions are available in the online Koblenz network collection\footnote{http://konect.uni-koblenz.de/}.
The aim of these experiments is to illustrate the advantage of the online inference and to evaluate the performances of the models.

\begin{table}[h]
\bgroup
\def\arraystretch{1} % 1 is the default, change whatever you need
	\input{img/corpus}
\egroup
\label{table:corpus}
\end{table}



%
% Evaluation method (testing set and mesure)
%
\paragraph{Performance Analysis} For testing the impact of the amount of data on the inference, we designed an experiment based on different sizes of training set. The evaluation is driven by  the missing link prediction task and the metric used is AUC-ROC score. For weighted models, we considered the probability that an edge exists between two unobserved nodes (i,j) belonging to the test set as the probability that they have at least one edge count defined by $p(y_{ij} \geq 1 | \Thetah, \Phih) = 1 - \sum_{kk'} \thetah_{ik} \thetah_{jk'} e^{\phih_{kk'}}$.

For all the datasets, we built a test set by extracting randomly 20 percent of the edges of the network and about the same amount of non-links. The remaining data constitutes the "full" training set. Then, we sub-sampled this full training set in order to obtain smaller sub-training sets (subgraphs) containing different proportions of the edges (i.e 1, 5, 10, 20, 30, 50, and 100 percent). Note that we ensure that all the sub-training sets are inclusive. We repeated this sampling 10 times with different seeds to cross validate our results, and the average values (and standard deviations) computed on the ten sub-training sets are reported, for each proportion, as final results.

% stopping citerion
For the stopping criteria of the inference, we  held-out 10 percent of the remaining (sub) training set and used it as a validation set. After each minibatch iteration,  the log-likelihood is computed on this validation set. When the increase of the log-likelihood on average (on the last 20 measures) is less than 0.001, the inference is stopped. The log-likelihood of a given set of observations $\D_{set}$  is given by:
\begin{equation*}
\log p(\D_{set}) = \sum_{i,j \in \D_{set}} \log p(y_{ij} | \phih_{kk'}) p(k|\thetah_i) p(k'|\thetah_j)
%\log p(\D_{test}) = \sum_{i,j \in \D_{test}} \log p(y_{ij} | \phih_{kk'}) p(k|\thetah_i) p(k'|\thetah_j)
\end{equation*}

For all our models, the gradient step parameters  $\tau$ and $\kappa$ were fixed respectively to  $1024$ and $0.5$, the burn-in period $T_{burnin}$ to $150$ and for the stratified sampling, the size of the non-edges set  $M$ was set to $50$. For MMSB, the hyperparameters $\lambda_0$ and $\lambda_1$ were set to $0.1$ and for WMMSB, the shape and scale parameters $r$ et $p$ were fixed to 1. The number of latent classes $K$ was fixed to $10$ for all models and the latent-class hyperparameters $\alpha_k$ to $\frac{1}{K}$.


Our models have been compared with two baselines, namely SBM and WSBM that are the non hierarchical counterpart of MMSB and WMMSB and which constitute standard approaches for link prediction. For both baselines, the microcanonical stochastic block model implementation of \cite{peixoto2018nonparametric} has been used since it integrates an efficient MCMC inference method for the stochastic block models family. This implementation can be found in the graph-tool framework \cite{peixoto_graph-tool_2014}. The number of classes was again set to $K=10$ for both models.


\textcolor{white}{ % trick to make it invisible, only in the footpage.
\footnote{\url{http://konect.uni-koblenz.de/networks/ca-AstroPh}. We used the cleaned version available in the  graph-tool framework.}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/ca-cit-HepTh}. We used the cleaned version available in the  graph-tool framework.}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/moreno_names}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/radoslaw_email}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/opsahl-ucsocial}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/munmun_digg_reply}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/slashdot-threads}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/enron}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/link-dynamic-simplewiki}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/prosper-loans}}
}
\begin{figure}[h]
\centering
	\input{img/roc_evolv_fig2}
   \label{fig:roc}
\end{figure}


Figure \ref{fig:roc} represents the AUC performances of models depending on the proportion of the training edges used for the inference. Not surprisingly, one can observe that they increase with the number of data used to learn. This comportment is particularly pronounced for SBM and WSBM because their inference algorithm is designed to learn on batch datasets, while our models converge faster to their best performances, due to the strong averaging step of the stochastic gradient descent, before overfitting eventually the training set.

WWMSB-bg outperforms WWMSB which highlights the importance of the gamma prior parameters in this model. Further, WMMSB-bg is competitive with the baselines and, it outperforms other models when the number of missing links is high (i.e. the number of training edges is lower or equal than 20 percent. See also column 10 in table \ref{table:roc}).
 
MMSB is competitive on some datasets (astro-ph, hep-th, moreno\_names) while it fails on the others. Interestingly, one can see than on the datasets where it fails (digg-reply, prosper-loans, slashdot and wiki-link), the variance is very large. This behavior could be exploited to indicate if the model is suited for capturinfg the structure of a given network. Furthermore, those datasets have a low density compared to astro-ph, hep-th and moreno\_names. Thus, this poor performance can be explained by the fact that the convergence of the model is very sensitive to the sampling choices done during the online inference. Indeed, sparse networks are known to exhibit a degree distribution following a power law, which implies that some nodes do not have the same importance but the sampling does not integrate this property. In this regard, WMMSB and WMMSB-bg models are less sensitive for those networks and are still competitive. This result confirm  that taking into account the edge covariates makes the weighted models less sensitive to the minibatches sampling in sparse networks. Finally, it worth to mention that on the dataset prosper-loans, which is the only network classified as "Interaction" network in the Konnect repository, the baselines failed to learn the topology of the networks and MMSB barely exceeds a random classifier; only WMMSB-bg succeeds with the best performance above 0.75.



% Table Roc comments
\begin{table}
\centering
	\input{img/roc_evolv_tab}
\label{table:roc}
\end{table}




%
% Convergence commments
%
\paragraph {Convergence Analysis} We studied in a deeper way the convergence of WMMSB and WMMSB-bg models on the different datasets. Figure \ref{fig:conv_entropy} shows the convergence of the log-likelihood on a test set composed of 20 percent of the edges  and about the same amount of non-edges for each network. We used three different sets for the hyperparameters shape $r$ and scale $p$ of WMMSB. Regardless the values of these hyperparameters, one can observe that the augmented model WMMSB-bg is less prone to overfitting and that for most of the networks, it outperforms its counterpart in terms of log-likelihood. Furthermore, one can see that this model needs a low proportion of the total number $N**2$ of edges to converge.

\begin{figure}[h]
\centering
	\input{img/conv_entropy3}
    \label{fig:conv_entropy}
\end{figure}

While our implementation is in python, the time convergence of the algorithm is fast and it is even comparable with SBM which is implemented in C. Furthermore, in practice, the algorithm exhibits a sublinear time complexity with the numbers of edges, as shown Table \ref{table:time}.

\begin{table}[h]
\begin{tabular}{llllll}
\hline
                    & MMSB                  & WMMSB-bg              & SBM                  \\
\hline
astro-ph      & 361.165 $\pm$ 159.107    & 868.802 $\pm$ 870.965    & 44.646 $\pm$ 4.778      \\
hep-th        & 170.281 $\pm$ 17.246     & 226.887 $\pm$ 77.84      & 20.366 $\pm$ 4.127      \\
moreno\_names  & 44.821 $\pm$ 7.717       & 30.362 $\pm$ 11.811      & 5.358 $\pm$ 1.67        \\
fb\_uc         & 50.927 $\pm$ 17.694      & 66.921 $\pm$ 10.439      & 4.093 $\pm$ 0.907       \\
digg-reply    & 1803.779 $\pm$ 1100.227  & 2044.334 $\pm$ 1016.157  & 228.661 $\pm$ 81.611    \\
slashdot      & 4272.667 $\pm$ 1971.346  & 2867.998 $\pm$ 653.092   & 365.437 $\pm$ 82.475    \\
enron         & 4660.744 $\pm$ 2978.898  & 3544.711 $\pm$ 385.445   & 1313.841 $\pm$ 109.074  \\
wiki-link     & 13683.093 $\pm$ 6890.663 & 5592.084 $\pm$ 228.13    & 613.841 $\pm$ 113.047   \\
prosper-loans & 6595.175 $\pm$ 4612.049  & 12255.827 $\pm$ 4878.113 & 1182.409 $\pm$ 183.342  \\
\hline
\end{tabular}
\label{table:time}
\caption{Inference time in seconds.}
\end{table}



%
% WSim
%
%For the weighted models, we further measure the capacity to predict right edge counts with a $l_1$ distance between the real count of the test set and the expected count of the models 
%
%\begin{equation*}
%D_{l_1}(D_{test} ||  \{\Thetah, \Phih\}) = \sum_{i,j \in \D_{test}} | y_{ij} - \E[y_{ij}|\Thetah, \Phih] |
%\end{equation*}






