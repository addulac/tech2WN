\section{Experimental validation}
\label{sec:exps}

%
% Corpus
%


We experimented our models and inference on several real world, either directed or undirected, networks. Theirs statistics and properties are summarized in table \ref{table:corpus}. The detailed descriptions are available in the online Koblenz network collection\footnote{http://konect.uni-koblenz.de/}.

\begin{table}[h]
\bgroup
\def\arraystretch{1} % 1 is the default, change whatever you need
	\input{img/corpus}
\egroup
\label{table:corpus}
\end{table}



%
% Evaluation method (testing set and mesure)
%
To evaluate the performance of the models and to illustrate the advantage of the online inference, we designed a experiments based on different size of training set to test the impact of the amount of data on the inference. For all the dataset, we first built a test set  by extracting randomly 20 percent of the edges of the networks and about the same amount of non-links. The remaining data consitute the "full" training set. Then, we subsampled the full training set in order to obtain "sub" training set containg different propotions of the edges (i.e 1, 5, 10, 20, 30, 50, and 100 percent, the former corresponfing to the full trainig set.). Furthermore, we ensure that all the sub training set are inclusive. We repeated this settings 10 times with different seed, to cross validate our results.

The all our models implemented with the stochastic collapsed variationnal inference, we set the gradient step parameters to $\tau=1024$ and $\kappa=0.5$, the burn-in period to $T_{burnin}=150$ and for the stratidied sampling, we fix the paramter $M=50$. For MMSB we set the hyperparameters $\lambda_0=\lambda_1=0.1$ and for MMSB, WMMSB and WMMSB-bg we set the latent-class hyperparameters to $\alpha_k=\frac{1}{K}$ and the number of latent classes to $K=10$.

% stopping citerion
For the stopping criteria of the inference, we further held-out 10 percent of the remaining training set is held-out during the inference. The log-likelihood is computed at after each minibatche. When the log-likelihood average increase (on the last 20 measures) is less that 0.001, the inference is stopped. The log-likelihood of a  given set of observations $\D_{test}$  is computed as follows:
\begin{equation*}
\log p(\D_{test}) = \sum_{i,j \in \D_{test}} \log p(y_{ij} | \phih_{kk'}) p(k|\thetah_i) p(k'|\thetah_j)
%\log p(\D_{test}) = \sum_{i,j \in \D_{test}} \log p(y_{ij} | \phih_{kk'}) p(k|\thetah_i) p(k'|\thetah_j)
\end{equation*}



\textcolor{white}{ % trick to make it invisible, only in the footpage.
\footnote{\url{http://konect.uni-koblenz.de/networks/ca-AstroPh}. We used the cleaned version available in the  graph-tool framenetwork.}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/ca-cit-HepTh}. We used the cleaned version available in the  graph-tool framework.}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/moreno_names}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/radoslaw_email}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/opsahl-ucsocial}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/munmun_digg_reply}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/slashdot-threads}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/enron}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/link-dynamic-simplewiki}}
\footnote{\url{hhttp://konect.uni-koblenz.de/networks/prosper-loans}}
}


\begin{figure}[h]
\centering
	\input{img/roc_evolv_fig}
   \label{fig:roc}
\end{figure}


In order to carachterize to wich extent the models can predict the missing links of the networks, an thus recover its topology. For the weighted models, the probability to observe a links is computed as the probability that at least one edge count is generated by:
\begin{equation*}
p(y_{ij} \geq 1 | \Thetah, \Phih) = 1 - \sum_{kk'} \thetah_{ik} \thetah_{jk'} e^{\phih_{kk'}}
\end{equation*}

The models are compared against two baselines, namely SBM and WSBM that are the non hierarchical counterpart of MMSB and WMMSB. Futhermore, we used the microcanonical stochastic blockmodel of \cite{peixoto2018nonparametric} that introduces an efficient MCMC inference method for the stochastic blockmodels, that are implemented in the graph-tool framework \footnote{\url{peixoto2018nonparametric}}.

In figure \ref{fig:roc}, we compared the performance of the models in terms of AUC-ROC on the different datasets. Generally,  We can see that performance increases with the number of oberved edges (i.e the size of the dataset.). This comportment particularcy pronouced for SBM and WSBM because there inference algorithm is designed to learn on batch dataset, while our models converge faster to their best performance  before it eventually overfit the trainig set. MMSB has a very high variance compared to other models, and while it can be competitive on some dataset (astro-ph, hep-th, moreno-names) it failed on other (propser-loan, slashdot, wiki-link. WWMSB-bg outperforms WWMSB and WMMSB-bg is competitive with the baselines. Furthermore, it outperforms the baseline when the number of missing links is high (1, 5 10 percent of the oberved edge) while its results stay stable between the full training set and the small one.



% Table Roc comments
\begin{table}
\centering
	\input{img/roc_evolv_tab}
\label{table:roc}
\end{table}
In table \ref{table:roc} we show precise results for 10 and 100 percent of the trainig set. It highlight the \textcolor{red}{to complete}


\textcolor{red}{add convergence results and comments}









%
% Convergence commments
%

%\begin{figure}[h]
%\centering
%	\input{img/conv_entropy}
%    \label{fig:conv_entropy}
%\end{figure}

%
% Sim
%
%For the weighted models, we further measure the capacity to predict right edge counts with a $l_1$ distance between the real count of the test set and the expected count of the models 
%
%\begin{equation*}
%D_{l_1}(D_{test} ||  \{\Thetah, \Phih\}) = \sum_{i,j \in \D_{test}} | y_{ij} - \E[y_{ij}|\Thetah, \Phih] |
%\end{equation*}






