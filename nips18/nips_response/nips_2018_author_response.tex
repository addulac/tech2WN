\documentclass{article}

\usepackage{nips_2018_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{lipsum}

\begin{document}

We wholeheartedly thank the reviewers for the time they have dedicated to this submission as well as for their insightful comments and stimulating questions, to which we reply below.

\paragraph{Modeling assumptions and conclusions}
%The reviewers pointed out some concerns about the experimental evaluation and the interpretation of the results. We give here more explicit statements and explanations of the modeling assumptions, theirs articulations with the experimental results and their interpretation.
We agree that the modeling assumptions for the weighted networks are somewhat implicit in the manuscript. The choice of the experimental design is oriented to assess to what extent the information on the edge weights can improve the link prediction problem. This choice is motivated by the following assumption:
%
\begin{itemize}
\item Assumption 1 (weighted preferential attachment): the probability to observe a new edge, for a given node, increases with its weighted degree. 
\end{itemize}
%
The standard assumption for preferential attachment however takes the following form:
\begin{itemize}
\item Assumption 2 (binary preferential attachment): the probability to observe a new edge, for a given node, increases with its (unweighted) degree.
\end{itemize}
%
Assumption 2 has been validated in different studies. We do not directly validate Assumption 1 here, but conjecture that when information regarding the degree of a node is scarce (\textit{i.e.} when few edges are observed), taking into account the weight information provides useful additional information, along the line of Assumption 1.
%The assumptions 1 and 2 are contradictory as phrased but, one can relax the assumptions by considering that the statements are true only asymptotically which make them compatible. Thus, if a statically sufficient number of edges are observed, one assume that the weights won't bring any useful information to predict the probability of a new edge. In the opposite, until a sufficient number of edges are observed, the weighted preferential attachment holds. Two consequences can be drawn from those assumptions; From assumption 1, for a sufficient small subset of the networks, the information of the observed weights should improved the missing edge prediction against the knowledge of the weightless edge only. From assumption 2, for sufficient large subset of the networks, the information of the weights should not improve the new edge prediction.
%
Our experimental results support this claim and suggest the following:
%
\begin{itemize}
\item Comparing the performance of the weighted models against SBM, one can see that weighted models outperform SBM, in the low data regime, for 8 out of 9 networks, all the models being on a par for the 9th network (\textit{Enron}). Yet, as pointed out by Reviewer \#2, MMSB outperforms the weighted models (again in the low data regime) on two networks, namely \textit{hep-th} and \textit{astro-ph}. This said, the results on these two networks should be interpreted with caution as they contain a bias that can penalize the weighted models. Indeed, for these co-authorship networks, the weights between two co-authors are computed as the number of papers they have in common normalized by the numbers of co-authors. As the number of co-authors of a given paper is unknown, we assumed a fixed number of 10 co-authors for all papers in order to obtain integer weights.
\item Comparing again SBM with the weighted networks, one can see that, for 8 out the 9 networks, when a sufficient number of edges are observed, SBM either converges to the performance of weighted models or outperforms them, in line with Assumption 2. This said, on the \textit{slashdot} dataset (the only network based on money loan interactions), all non-weighted models fail to accurately predict new links, contrary to say WMMSB. This shows that, on certain network types, weights can be important to accurately predict new links.
\item We evaluated all models on a variety of network types in order to avoid selecting a set of networks that would benefit only weighted or unweighted models. If this variety makes it more difficult to analyze precisely what types of networks may benefit from modeling weights, our study still reveals that it is important to take into account weights when few edges are available or if one wants to subsample a network for scalability reasons.
\end{itemize}

\paragraph{The Poisson assumption} A justification was asked by Reviewer \#2 on why we did not consider two different distributions for the modeling of \emph{whether or not} an edge exists between nodes and the count observed at that edge if it exists. We considered that the Poisson law is a natural extension of the Bernoulli law to represent a discrete number of interactions between nodes. Hence, when predicting whether or not an edge exists between two nodes for weighted models, we simply evaluate the probability that the weight on that edge is greater than zero. We believe that this assumption can be justified by the intuition (at least in the networks we evaluated) that if there exists at least one edge between two nodes, then an edge is assumed to exist in the underlying "binary network".

\paragraph{Baseline models and weight prediction} We agree that the experimental section can be strengthened by adding recent baselines for weighted networks, such as the EPM model, as mentioned by Reviewer \#3, as well as a weight prediction study. We will integrate that in the next version of the paper.

\paragraph{Performance metrics}  AUC-PR yielded results equivalent to AUC-ROC; we thus reported only the AUC-ROC results. This is due to the construction of the training and test sets, as described in the paper, in which the number of edges and non-edges are similar (20\% percent of the edges and about the same amount of non edges).

\paragraph{Hyper-parameter K} We fixed K=10 in this study to have a first comparison \textit{mutatis mutandis} between the different models (the latent classes play the same role in all models, even though the assignment is hard for SBM and soft for MMSB-like models). This said, we agree that it is also important to cross-validate K for each model and compare the results obtained by doing so. We will do that.

\end{document}
