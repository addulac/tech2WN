%\section{Weighted Networks}
%
%Most of real networks exhibit a topology more complex than just binary relationship between nodes. Instead, the relations can be weighted and dynamic. For example, co-authorship networks can be constructed such that the edges covariate corresponds to the number of collaborations between the corresponding authors \cite{newman2001scientific}. In a communication network, the weight can be the number of messages sent from the sender to the receiver. In the web, documents are connected with hyperlinks where the count of those is for example used to construct the PageRank algorithm. Finally, in a linguistic network, a network of words can be built where the weight between two words is the number of times where they follow each other. Another useful case where weighted networks can be useful is temporal networks. For instance, in communication networks, messages are sent at a specific time, thus taking into account the number of messages send during a period allows to represent the strength of the relation over the time.
%
%
%A natural prior for count edge covariate is the Poisson distribution. In addition, it has several nice properties,
%%\cite{orbanz2015bayesian}
%
%\begin{itemize}
%\item{Additivity}: If $K_1 \sim \mathrm{Poi}(\alpha_1)$ and $K_2 \sim \mathrm{Poi}(\alpha_2)$ then,
%    \begin{equation*}
%        K_1 + K_2 = \mathrm{Poi}(\alpha_1 + \alpha_2)
%    \end{equation*}
%\item {Thinning}: The number of successes in a Poisson number of coin flips is Poisson, namely if $K \sim \mathrm{Poi}(\alpha)$ and $X_1,...,X_K \sim \mathrm{Bern}(p)$ then,
%    \begin{equation*}
%        \sum_{i=1}^K X_i = \mathrm{Poi}(p\alpha)
%    \end{equation*}
%\end{itemize}
%
%These two properties justify to build weighted networks datasets from sequence of either weighted graphs or binary graphs to feed a Poisson based model. In the rest of the paper we will assume that a network is represented by a graph $G=(V,E)$ where $V$ is the set of nodes such that $N=|V|$ and E the set of edges. We consider the adjacency matrix $Y=(y_{ij})_{ij\in N^2}$ such that $y_{ij}=0$ if $(i,j) \notin E$ and $y_{ij} > 0$ otherwise.
%
%In the rest of the paper, we will use the notation $n^{-ij}$ to indicate that the superscript $ij$ is excluded from the underlying count variable, and $n_{\bm{.}}$ to indicate a sum over the dotted subscript index.

\section{Mixed-membership stochastic block models and (un)weighted graphs}

As usual, we consider here that a network is represented by a graph $G=(V,E)$ where $V$ is the set of nodes such that $N=|V|$ and E the set of edges. We consider the adjacency matrix $Y=(y_{ij})_{ij\in N^2}$ such that $y_{ij}=0$ if $(i,j) \notin E$ and $y_{ij} > 0$ otherwise.

Mixed-Membership Stochastic Block (MMSB) models extend stochastic block models \cite{airoldi2009mixed} by allowing nodes to "belong" to several blocks (or classes) through a given (usually Dirichlet) probability distribution. Prior to generate a link between two nodes, a particular class is selected for each node. The link is then generated according to a probability distribution $F$ that depends on the selected classes. The generative process behind such models can be summarized as: (a) For each node $i$, draw $\theta_i \sim \textrm{Dir}(\alpha)$, where $\theta_i$ and $\alpha$ are $K$-dimensional vectors, where $K$ denotes  the number of classes considered; (b) Generate two sets of latent class memberships, $Z_\rightarrow = \{z_{i\rightarrow j} \sim \textrm{Cat}(\theta_i),  1 \le i,j \le N\}$ and $Z_\leftarrow = \{z_{i\leftarrow j} \sim \textrm{Cat}(\theta_j),  1 \le i,j \le N\}$, with categorical draws; (c) Generate or not a link between two nodes $(i,j)$ according to $y_{ij} \sim F(\phi_{z_{i \rightarrow j}z_{i \leftarrow j}})$, where $F$ is a distribution in the exponential family and $\phi_{z_{i \rightarrow j}z_{i \leftarrow j}}$ an associated (usually conjugate) distribution that represents the relations between classes. For unweighted graphs, $F$ is the Bernoulli distribution and $\phi$ its conjugate Beta distribution.

Many real networks nevertheless rely on graphs in which edges are naturally weighted. In co-authorship networks, for example, it is standard to consider edges weighted according to the number of collaborations between authors \cite{newman2001scientific}. In communication networks, the weight are based on the number of messages sent from the sender to the receiver. In text mining and natural language processing applications, it is also common to use word graphs in which edges are weighted on the basis of the number of times the words co-occur (in a sentence, paragraph or document). In all these cases, weights are integers that can naturally be modeled with Poisson distributions. Relying on its conjugate Gamma distribution for $\phi$, one finally obtains the following models, denoted MMSB for unweighted graphs and WMMSB for weighted graphs:
%
\[
\theta_i \sim \textrm{Dir}(\alpha), \,\, z_{i\rightarrow j} \sim \textrm{Cat}(\theta_i), \,\, z_{i\leftarrow j} \sim \textrm{Cat}(\theta_j)
\]
%
and:
%
\begin{align*} \label{eq:generative}
y_{ij} &\sim \textrm{Bern}(\phi_{z_{i \rightarrow j}z_{i \leftarrow j}}), &\phi_{kk'} &\sim \textrm{Beta}(\lambda_0, \lambda_1), & \textrm{  \textit{for unweighted graphs (MMSB)}} \\
y_{ij} &\sim \textrm{Poi}(\phi_{z_{i \rightarrow j}z_{i \leftarrow j}}), &\phi_{kk'} &\sim \textrm{Gamma}(r, \frac{p}{1-p}),    & \textrm{  \textit{for weighted graphs (WMMSB)}} 
\end{align*}
%
The choice made here for the Gamma distribution in WMMSB allows one to represent overdispersed count data as one has $y_{ij} \sim \textrm{NB}(r,p)$ \cite{zhou2012beta}, where $\textrm{NB}$ denotes the negative binomial distribution. Furthermore, the above models are valid for both directed and undirected graphs, the matrix $\Phi = (\phi_{kk'})_{k,k' \in \{1,..,K\}^2}$ being symmetric in the latter case.

%In the following we will denote the set of the model parameters $\Pi = \{ \Theta, \Phi, Z \}$ with $\Theta = (\theta_{ik})_{i,k \in \{1,..,N\}\times \{1,..,K\}}$ and the set of model hyper-parameters $\Omega$.

\subsection{Beta-Gamma augmentation}

The generative process for WMMSB defined above assumes that the parameters of the Poisson distributions used to generate links are drawn from the same Gamma distribution. Having a unique prior over these parameters however limits the ability of the model to capture the variance in the relations between the latent classes. Hierarchical extensions can be used here to have a better representation of the classes and the relations between them. Following the Beta-Gamma-Gamma-Poisson model \cite{zhou2012beta} and the Gamma-Negative Binomial process \cite{zhou2015negative}, we model here the rate parameter of the Gamma distribution used in WMMSB with a Beta prior and its shape parameter with another Gamma distribution of the form:

\begin{gather*}
r_{kk'} \sim \textrm{Gamma}(c_0r_0, 1/c_0) \qquad p_{kk'} \sim \textrm{Beta}(c\epsilon, c(1-\epsilon)) \\
\phi_{kk'} \sim \textrm{Gamma}(r_{kk'}, \frac{p_{kk'}}{1-p_{kk'}})
\end{gather*}

The hyperparameters of this model, denoted as bg-WMMSB, are: $\Omega = (\alpha, c_0, r_0, c, \epsilon)$. The variable $y_{ij}$ is again distributed according to a negative binomial distribution, of the form: $y_{ij}|_{Z} \sim \textrm{NB}(r_{z_{i \rightarrow j} z_{i \leftarrow j}},p_{z_{i \rightarrow j} z_{i \leftarrow j}})$. As one can note, and contrary to WMMSB, the parameters of the negative binomial distribution depend this time on the classes selected for each node, meaning that classes know play a prominent role in the model.

As for most hierarchical Bayesian model, exact inference is intractable and one must resort to approximate inference. In the next section we propose a stochastic collapsed variational inference algorithm for the above models (MMSB, WMMSB, bg-WMMSB) that can be used in online settings.

\section{Inference}

One drawback of the original inference method proposed for MMSB models is its quadratic complexity in the number of nodes. It is impossible to use such a method on large networks. Recent advances in Stochastic Variational Inference (SVI) \cite{hoffman2013stochastic}, notably based on well designed sampling techniques \cite{gopalan2013efficient,kim2013efficient}, address this challenge by enabling the control of the memory complexity with online/minibatch updates while keeping the time complexity linear in the number of edges present in the network. Such approaches are thus very well adapted to online settings, in which links are observed during certain time intervals, over sparse networks (real world networks are most of the time very sparse \cite{barabasi_burst}).

We propose to combine here SVI with Collapsed Variational Bayesian (CVB) inference \cite{teh2006collapsed} that relies on a weaker assumption on the variational distribution.

\subsection{Collapsed variational inference}

In the remainder, we use the notation $n^{-ij}$ to indicate that the superscript $ij$ is excluded from the underlying count variable, and $n_{\bm{.}}$ to indicate a sum over the dotted subscript index. Furthermore, $\Pi$ will denote the model parameters ($\Pi = (\Theta,\Phi,Z)$ for MMSB and WMMSB and $(\Theta,\Phi,Z,R,P)$ for bg-WMMSB) and $\Omega$ the hyperparameters ($\Omega = (\alpha,\lambda_0,\lambda_1)$ for MMSB, $(\alpha,r,p)$ for WMMSB and $(\alpha, c_0, r_0, c, \epsilon)$ for bg-WMMSB). From Jensen's inequality, for any distribution $q$, one has:
%
\begin{equation*}
\log p(Y | \Omega) \ge \E_{q}[\log p(Y, \Pi\ | \Omega)] + \textrm{H}[q(\Pi)]
\end{equation*}
%
where $\textrm{H}$ denotes the entropy. The goal of variational inference is then to find $q$ that maximizes the right-hand side of the above inequality, usually referred to as the Evidence Lower BOund (ELBO). In its collapsed version, following \cite{teh2006collapsed}, one weakens the mean-field assumption made over the variational distribution, leading to, for MMSB and WMMSB:
%
\begin{equation*}
q(\Pi) = q(\Theta, \Phi | Z) q(Z)
\end{equation*}
%
with $q(z_{i \rightarrow j}, z_{i \leftarrow j}|\gamma_{ij})$ being multinomial with parameter $\gamma_{ij}$. The evidence is then lower bounded by:
%
\begin{equation*}
\log p(Y|\Omega) \geq \underbrace{\E_{q}[\log p(Y, Z)] + \textrm{H}[q(Z)]}_{\L_Z}
\end{equation*}

Maximizing $\L_Z$ w.r.t $\gamma_{ijkk'}$ under a zero order Taylor expansion and a Gaussian approximation, following \cite{teh2006collapsed,asuncion2009smoothing}, yields the following updates, detailed in the supplementary material:
%
\begin{equation} \label{eq:maximization}
\gamma_{ijkk'} \propto (N_{\rightarrow ik}^{\Theta^{-j}} + \alpha_k) (N_{\leftarrow jk}^{\Theta^{-i}} + \alpha_{k'}) p(y_{ij} | Y^{\neg ij}, Z^{\neg ij}, \zij=k, \zji=k')
\end{equation}
%
where the elements $N^{\Theta}$ are defined in Eqs~\eqref{eq:sss}. Depending on the model considered, the predictive link distribution $p(y_{ij}|z_{i \rightarrow j}=k,z_{i \leftarrow j}=k')$ takes the following form:
%
\begin{align*}
p(y_{ij} | z_{i \rightarrow j} = k,z_{i \leftarrow j} = k') \begin{cases}
    = \left( \frac{ N^{\Phi^{-ij}}_{1 kk'} + \lambda_1}{N^{\Phi^{-ij}}_{\bm{.}kk'} + \lambda_{\bm{.}}}\right)^{y_{ij}} \left( 1- \frac{ N^{\Phi^{-ij}}_{1 kk'} + \lambda_1}{N^{\Phi^{-ij}}_{\bm{.}kk'} + \lambda_{\bm{.}}}\right)^{1-y_{ij}}  & \textrm{ for MMSB} \\
    \sim \mathrm{NB}\left(y_{ij}; N^{Y^{-ij}}_{kk'} + r, \frac{p}{p\,N^{\Phi^{-ij}}_{\bm{.}kk'} + 1} \right) & \textrm{ for WMMSB} % \\
%    \sim \mathrm{NB}\left(y_{ij}; N^{Y^{-ij}}_{kk'} + \E_{q}[r_{kk'}], \frac{\E_{q}[p_{kk'}]}{\E_{q}[p_{kk'}]\,N^{\Phi^{-ij}}_{\bm{.}kk'} + 1} \right) & \textrm{ for bg-WMMSB}
\end{cases}
\end{align*}

The different count statistics $N^*$ are estimated from the variational parameters $\gamma_{ijkk'}$ by:
%
\begin{align} \label{eq:sss}
    N^{\Theta}_{\rightarrow ik} &= \sum_{j, k'} \gamma_{ijkk'}        & N^{\Theta}_{\leftarrow jk'} &= \sum_{i, k} \gamma_{ijkk'}  \nonumber \\
    N^{\Phi}_{xkk'} &= \sum_{ij:y_{ij}=x} \gamma_{ijkk'}  & N^{Y}_{kk'} &= \sum_{ij} y_{ij}\gamma_{ijkk'}
\end{align}
%
In this inference scheme, $\gamma_{ij}$ are the \emph{local} parameters while the count statistics $N^*$ represent the \emph{global} parameters.  

Finally, the model parameters can be recovered from their estimates as follows:
%
\begin{align*}
\hat \theta_{ik} = \frac{N^{\Theta}_{\rightarrow ik} + N^{\Theta}_{\leftarrow ik} + \alpha_k}{2N + \alpha_{\bm{.}}} \qquad 
\hat \phi_{kk'}=\begin{cases}
     \frac{N^{\Phi}_{1 kk'} + \lambda_1}{N^{\Phi}_{\bm{.}kk'} + \lambda_{\bm{.}}} & \textrm{ for MMSB} \\
    \frac{p(N^Y_{kk'} + r)}{N^{\Phi}_{\bm{.}kk'} - p + 1}  & \textrm{ for WMMSB}  % \\
%    \frac{\E_{q}[p_{kk'}](N^Y_{kk'} + \E_{q}[r_{kk'}])}{N^{\Phi}_{\bm{.}kk'} - \E_{q}[p_{kk'}] + 1}  & \textrm{ for bg-WMMSB} 
    \end{cases}
\end{align*}

\subsubsection{Beta-Gamma augmentation}

For bg-WMMSB model, we consider the following collapsed variational distribution:
%
\begin{equation*}
q(\Pi) = q(\Theta, \Phi|Z, R, P)q(Z)q(R)q(P)
\end{equation*}
%
with $R=(r_{kk'})$ and $P=(p_{kk'})$, $k,k' \in \{1,..,K\}^2$. As before, $q(z_{i \rightarrow j}, z_{i \leftarrow j}|\gamma_{ij})$ is multinomial with parameter $\gamma_{ij}$. 

Following the same development as before, the parameters $\gamma_{ijkk'}$ are given by Eq.~\ref{eq:maximization}. Furthermore, $p(y_{ij}|z_{i \rightarrow j}=k,z_{i \leftarrow j}=k')$ and $\hat \phi_{kk'}$ now take the form:
%
\[
p(y_{ij} | z_{i \rightarrow j} = k,z_{i \leftarrow j} = k')  \sim \mathrm{NB}\left(y_{ij}; N^{Y^{-ij}}_{kk'} + \E_{q}[r_{kk'}], \frac{\E_{q}[p_{kk'}]}{\E_{q}[p_{kk'}]\,N^{\Phi^{-ij}}_{\bm{.}kk'} + 1} \right)
\]
\[
\hat \phi_{kk'} = \frac{\E_{q}[p_{kk'}](N^Y_{kk'} + \E_{q}[r_{kk'}])}{N^{\Phi}_{\bm{.}kk'} - \E_{q}[p_{kk'}] + 1}
\]
%
Exploiting the conjugacy of the Beta and the negative binomial distributions and assuming that $q(P)=p(P|Y,Z)$, one can show that $p_{kk'}$ can be sampled from a Beta distribution:
%
\begin{equation} \label{eq:pk_update}
p_{kk'} \sim \textrm{Beta}(c\epsilon + N^Y_{kk'}, c(1-\epsilon) + N^\Phi_{kk'}r_{kk'})
\end{equation}
%
so that: $\E_{q}[p_{kk'}] = \frac{c\epsilon + N^Y_{kk'}}{c\epsilon + N^Y_{kk'} + c(1-\epsilon) + N^\Phi_{kk'}r_{kk'}}$.

Lastly, by setting $q(r_{kk'}) = \textrm{Gamma}(a_{kk'},b_{kk'})$, one can show (see supplementary material) that $r_{kk'}$ can be sampled from a Gamma distribution ($r_{kk'} \sim \textrm{Gamma}(r_0c_0+N^Y_{kk'},\frac{1}{c_0  -N^\Phi_{kk'}\log(1-\E_{q}[p_{kk'}])})$ and is such that:
%
\begin{equation}
\E_{q}[r_{kk'}] = \frac{r_0c_0+N^Y_{kk'}}{c_0  -N^\Phi_{kk'}\log(1-\E_{q}[p_{kk'}])}
\end{equation}

\subsection{Stochastic variational inference with stratified sampling}

As mentioned before, we couple CVB with SVI. SVI aims at optimizing ELBO through noisy yet unbiased estimates of its natural gradient \cite{hoffman2013stochastic}. %The noise is associated from the training data that are randomly sampled when updating the variational parameters. 
Suppose  that we sample a dyad $(i,j)$ in the graph $G$ with a distribution $g(i,j)$, then SVI can be summarized as follows:
\begin{enumerate}
\item Maximize local parameter $\gamma_{ij}$ with equation \eqref{eq:maximization},
\item Compute the intermediate global parameter expectation: $\hat N = \frac{\gamma_{ij}}{g(i,j)}$,
\item Update the global parameter: $N^{(t+1)} = (1-\rho_t)N^{(t)} + \rho_t \hat N$.
\end{enumerate}

One bottleneck of the CVB updates is that it computes the expectation on the distribution $q(Z^{-ij})$. Consequently, the maximization step needs to keep in memory the $(\gamma_{ij})$ matrix of size $N^2\times K^2$ which is not convenient for large networks. Thus, following \cite{foulds2013stochastic}, we consider that for large count, the $ij^{th}$ term has a negligible impact on the overall sum and so we omit it. In fact, Foulds et al. shown that this approximation is equivalent to make a online EM MAP estimation.

An advantage of SVI is that it let the freedom to choose a sampling strategy. Stratified sampling was proposed in \cite{gopalan2013efficient} with a SVI inference for the binary MMSB model. We adapt this sampling strategy for our SCVI algorithm. Instead of sampling on a distribution $g(i,j)$ on edges, it samples on a subset $S$ of edges (i.e minibatches) with distribution $h(S)$. For each nodes, we build a link-set $S_1$ containing all its edges, and a non-link set $S_0$ of all its non-links that we scattered in $m$ subsets of equal size such that $S_0 = \cup_{n=1}^m S_{0,n}$. Then for each iteration, we randomly pick one node and either its set $S_1$ or one of its $m$ subset in $S_0$ with
\begin{align*}
h(S)=\begin{cases}
    \frac{1}{\chi N}  & \textrm{ if } S = S_1 \\
    \frac{1}{\chi N m}  & \textrm{ if } S \in S_0 
    \end{cases}
\end{align*}
Where $\chi$ is a factor to take into account the symmetry of the adjacency matrix which is fixed to 1 for directed graph and fixed to 2 otherwise. The sampling method allows to adapt the inference to the sparsity of the graph. Moreover, empirical results show that stratified sampling improves speed and quality of the convergence compared to other sampling strategy \cite{gopalan2013efficient}\cite{kim2013efficient}. In our experiments, we find out that sampling the minibatches in $S_0$ with replacement of the non-links improved the performance for both MMSB and WMMSB.
%Our sampling strategy differ from \cite{gopalan2013efficient}, in the distribution over the set $S_0$, since we sample the subset of non-links with replacement the two distribution differ of a factor $N$.

\textbf{Robbins-Monro Condition} The convergence of the SVI is guaranteed under the Robbin-Monro condition \cite{robbins1951stochastic} that imposes constraints on the gradient step, $\sum \rho_t = \infty$ and $\sum \rho_t^2 < \infty$ which can be obtained with $\rho_t = \frac{1}{(\tau +t)^\kappa}$ with $\kappa \in (0.5, 1]$. Thus, we maintain a gradient step for each of the global parameters $\rho^\Phi$ and $\rho^Y$ accounting respectively for  $N^\Phi$ and $N^Y$. For $N^\Theta$, we maintain individual gradient steps $\rho_i^{\Phi}$ for $1\leq i\leq N$, following \cite{miller2009nonparametric}; this improved both convergence and prediction performance.% Thus for each minibatch we only increase the gradient steps of the nodes that are observed. 
Furthermore, to increase the speed of the inference, we update the global parameter $N^\Phi$ and $N^Y$ only after a minibatch round. For the global parameter $N^\Theta$, we update it after a burn-in period $T_{burnin}$ such that $T_{burnin} \leq |S|$.
%, which consists to update the class assignment of nodes after observing a bunch of dyad.
This heuristic provides a trade-off between updating the global parameters after each observation, which slows down the inference and may result in bad local optima, and updating them only after minibatches that are potentially large (proportional to the number of nodes).

%%% Two more point exists:
% * the time step is increased with the minibatch size,
% * the intermiatade graidnet is normalized by the minibatch size.

%Our SCVI algorithm is summarized in pseudo-code \ref{algo:scvb}.
%
%\begin{algorithm}
%\KwIn{Random initialization of $N^\Theta, N^\Phi, N^Y$.}
%\KwOut{$\Thetah, \Phih$.}
%\Begin{
%$t \gets 0$ \\
%\While{Convergence criteria not met}{
%    Sample a minibatch $S_t$ from $h(S)$. \\
%    \ForEach{$i,j \in S_t$}{
%        Maximize local parameters $\gamma_{ij}$ from \eqref{eq:maximization}.\\
%        \If{burn-in finished}{
%            Compute intermediate gradient $\hat N^\Theta$ from \eqref{eq:sss}.\\
%            Update global parameter $N^{\Theta (t)}$.\\
%            Update gradient step $\rho^\Theta_t$.\\
%        }
%    }
%
%    Compute intermediate gradient $\hat N^\Phi$ and $\hat N^Y$  from \eqref{eq:sss}.\\
%    Update global parameters $N^{\Phi (t)}$ and $N^{Y (t)}$.\\
%    Update gradient steps $\rho^\Phi_t$ and $\rho^\Theta_t$.\\
%	Sample $P$ and $R$ from \eqref{eq:pk_update} \eqref{eq:rk_update}.\\
%    $t \gets t + 1$ .
%    }
%}
%\caption{SCVI pseudo-code.}
%\label{algo:scvb}
%\end{algorithm}

