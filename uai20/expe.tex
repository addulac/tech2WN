\section{Experimental validation}
\label{sec:exps}

We evaluated the predictive performances of our models on several real world networks, directed and undirected. Theirs characteristics and properties are summarized in Table \ref{table:corpus} and detailed descriptions are available in the online Koblenz network collection\footnote{http://konect.uni-koblenz.de/networks/}. For both astro-ph and hep-ph datasets, we used the cleaned version available in the  graph-tool framework.
%The aim of these experiments is to illustrate the advantage of the online inference and to evaluate the performances of the models.
This evaluation is based on a missing weight prediction task using the MSE score. 
For all the datasets, we built a test set by extracting randomly 20 percent of the edges of the network and about the same amount of non-linked pairs of nodes. The remaining data constitutes the training set. We repeated this sampling 10 times with different seeds to cross validate our results. The average values (and standard deviations) computed on the ten sub-training sets are reported as final results.

\begin{table*}[t]
\bgroup
\def\arraystretch{1} % 1 is the default, change whatever you need
	\input{img/corpus}
\egroup
\label{table:corpus}
\end{table*}

\subsection{Experimental setup}


We compared our models with several implementations of the state of the art. In the stochastic block model family, we considered the microcanonical stochastic block model implementation of \cite{peixoto2018nonparametric} which integrates an efficient MCMC inference method and its weighted extension, referred respectively to as SBM-gt and WSBM-gt. In the exponential family, we chose the SBM and WSBM implementations of \cite{aicher2014learning} which use a generic variational inference with several kernels, referred to as SBM-ai for the Bernoulli kernel, WSBM-ai-n for the Normal kernel and WSBM-ai-p for the Poisson kernel. Lastly, we compared also our models to the Edge Partition Model (EPM) proposed by ~\cite{zhou2015} which is based on a MCMC inference.


For the models belonging to the weighted mixed-membership class, WMMSB and EPM, we used the inferred posterior distribution to estimate the missing weights by:
\[
%p(y_{ij} \geq 1 | \Thetah, \Phih) = \sum_{kk'} \thetah_{ik} \thetah_{jk'} e^{-\phih_{kk'}}
\hat y_{ij} | \Thetah, \Phih = \sum_{kk'} \thetah_{ik} \thetah_{jk'} \phih_{kk'}
\]
Since the SBM models have been designed for solving the link prediction task, we do not have a posterior distribution adapted for weight prediction. Therefore, we used a post estimation of the average weight value in each classes interaction based on the observed data. More formally, given $O_{kk'}$, a set of observable links (and non links) between the inferred latent classes $k$ and $k'$, the prediction of a missing link between the classes $k$ and $k'$ is given by:
\[
\hat y_{ij} | e_i \in \text{class } k, e_j \in \text{class }k' = \sum_{y_{ij} \in O_{kk'}} \frac{y_{ij}}{\# O_{kk'}}
\]

Note that we used the same method for the weighted versions of the Aicher and Peixoto SBM models as we observed no significant difference and even better results using this latter estimation.

%For all the datasets, we built a test set by extracting randomly 20 percent of the edges of the network and about the same amount of non-links. The remaining data constitutes the training set. We repetead this sampling 10  times with different seeds to cross validate our results. The average values (and standard deviations) computed on the ten sub-training sets are reported, for each proportion, as final results.

% stopping citerion
For deciding when to stop the inference process, 10\% of the training set serves as a validation set on which the log-likelihood is computed after each minibatch iteration. When the increase of the log-likelihood, averaged over the last 20 measures, is less than 0.001, the inference is stopped. The log-likelihood of a given set of observations $\D_{set}$  is given by:
\begin{equation*}
\log p(\D_{set}) = \sum_{i,j \in \D_{set}} \log p(y_{ij} | \phih_{kk'}) p(k|\thetah_i) p(k'|\thetah_j)
%\log p(\D_{test}) = \sum_{i,j \in \D_{test}} \log p(y_{ij} | \phih_{kk'}) p(k|\thetah_i) p(k'|\thetah_j)
\end{equation*}

For all our models, the gradient step parameters  $\tau$ and $\kappa$ were fixed respectively to  $1024$ and $0.5$, the burn-in period $T_{burnin}$ to $150$; for stratified sampling, $M$ was set to $50$, the size of $s_0^{i,m}, \, 1 \le m \le M$ being equal to the number of nodes to which $i$ is not connected to divided by $M$. For MMSB, the hyperparameters $\lambda_0$ and $\lambda_1$ were set to $0.1$. For WMMSB, the shape and scale parameters $r$ et $p$ were fixed to $1$ and for WMMSB, the beta-gamma hyperparameters were fixed to  $c_0=10$, $r_0=1$, $c=100$ and $\epsilon=10^{-6}$. The number of latent classes $K$ was fixed to $10$ for all models and the latent-class hyperparameters $\alpha_k$ to $\frac{1}{K}$. Our implementation is available online\footnote{https://github.com/***/*** (anonymized)}. 


\begin{table*}[t]
\centering
	\input{fig2/wsim_training100}
\label{table:mse}
\end{table*}

\subsection{Results}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Prediction performance
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Table \ref{table:mse}, gives the MSE scores for the different models and for all networks. To evaluate the ability of the models to reconstruct the missing weights, we compute the MSE only on the missing edges of the test data. Although the models are also able to predict the presence or absence of edges,  we considered that the question of recovering the edges structure should be addressed separately by a link prediction method. 
As one can note, WMMSB-bg outperforms all the other models on the different datasets except for the network moreno\_names. 
The fact that this network is the only one in the category of linguistic network and that it is relatively small could explain this result. \textbf{CL : je ne suis pas d'accord avec phrase suivante} EPM outperforms other models, apart from WMMSB-bg. The other models expose comparable results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Sensibility on K 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 

\begin{figure}[h]
\centering
	\input{fig2/wsim_k_evo}
   \label{fig:k_evolv}
\end{figure}
Figure \ref{fig:k_evolv} shows the MSE score for $K$, the number of latent classes, varying from 10 to 50 and for six datasets. One can see that for the different methods, the result is relatively stable, in a lesser extent on the networks hep\_th and fb\_uc. Thus, the choice of this parameter has a limited impact on the performances of the models. Nevertheless, it should be notice that for $K$ equals  to 50, EPM has not been able to handle the datasets.

\textbf{CL: Pbl axe 2 indique Wsim et non MSE}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Timing performance
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
	\input{fig2/timing_100_k10.tex}
\label{table:time}
\end{table*}
Table \ref{table:time} presents the inference time of the models on the different dataset in hours. The models MMSB-scvb and WMMSB-gt are implemented in Python, SBM-ai-b/WSBM-ai-n/WSBM-ai-p in Matlab and SBM-gt and WSBM-gt in C++ (with a Python wrapper \cite{peixoto_graph-tool_2014}), which must be considered when comparing the timing. Furthermore, a hard limit  was set at 25 hours for the inference time to limit the computing cost. We can observe that SBM-gt is the fastest model while EPM is the slowest. This last one can take more than 20 times the inference duration of MMSB and WMMSB; which confirms its incapacity to handle the networks with $K$ equals 50 as seen previously. Notably, the inference of MMSB and WMMSB with the SCVB inference scheme fits millions of edges in less than 2 hours, while it is implemented in Python, which is very satisfying and allows to expect better performances  with optimized implementations for this inference scheme.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Convergence of *MMSB SCVB inference
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Figure \ref{fig:conv_entropy} shows the evolution of the log-likelihood for the MMSB-based models on the test set. We used three different sets for the hyperparameters shape $r$ and scale $p$ of WMMSB. Regardless of the values of these hyperparameters, one can observe that the augmented model WMMSB-bg is less prone to overfitting, usually converges to a better solution and only needs a small proportion of the total number $N^2$ of edges to do so.
\textbf{CL : detail sur les 3 ensembles a preciser ?} 

\begin{figure}[h]
\centering
	\input{img/conv_entropy3}
    \label{fig:conv_entropy}
\end{figure}







