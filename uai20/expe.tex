\section{Experimental validation}
\label{sec:exps}

We experimented our models on several real world networks, directed and undirected. Theirs statistics and properties are summarized in table \ref{table:corpus} and detailed descriptions are available in the online Koblenz network collection\footnote{http://konect.uni-koblenz.de/networks/}. For both astro-ph and hep-ph datasets, we used the cleaned version available in the  graph-tool framework.
%The aim of these experiments is to illustrate the advantage of the online inference and to evaluate the performances of the models.

\begin{table*}[t]
\bgroup
\def\arraystretch{1} % 1 is the default, change whatever you need
	\input{img/corpus}
\egroup
\label{table:corpus}
\end{table*}

\subsection{Experimental setup}

Our evaluation is based on a missing weight prediction task using a MSE score. We compared our proposed models with several implementation of stochasticl block models; The microcanonical stochastic block model implementation of \cite{peixoto2018nonparametric} which integrates an efficient MCMC inference method for the stochastic block model family stochastic block model, and its weighted extension, referred respectively to as SBM-gt and WSBM-gt; The SBM and WSBM implementations of \cite{aicher2014learning} which used a generic variational inference for the exponential family, referred to as SBM-ai for the Bernoulli kernel, SBM-ai-n for the normal kernel and SBM-ai-p for the Poisson kernel. Lastly, the Edge Partition Model (EPM) proposed by ~\cite{zhou2015infinite} which is based on a MCMC inference referred to as EPM.

For the models in the mixed-membership class (*MMSB and EPM), we used the infered posterior distribution to estimate the missing weights. Thus for *MMSB model, we have
\[
%p(y_{ij} \geq 1 | \Thetah, \Phih) = \sum_{kk'} \thetah_{ik} \thetah_{jk'} e^{-\phih_{kk'}}
p(y_{ij} | \Thetah, \Phih) = \sum_{kk'} \thetah_{ik} \thetah_{jk'} \text{Poi}(\phih_{kk'})
\]

Since the SBMs are based on link prediction, we do not have a posterior distribution adapted for weight prediction. Therefore, we used a post estimation of the average weight value in each class interaction based on the observed data. That is, let $O_{kl}$ be a set of observable links (and non links) between the inferred latent class $k$ and $l$, the prediction of a missing link between the class $l$ and $k$ is given by
\[
\hat y_{ij} | e_i \in \text{class } l, e_j \in \text{class }l  = \sum_{y_{ij} \in O_{kl}} \frac{y_{ij}}{\# O_{kl}}
\]

Note that we used the same method for the weighted version of the SBM models of the Aicher and Peixoto model. 

For all the datasets, we built a test set by extracting randomly 20 percent of the edges of the network and about the same amount of non-links. The remaining data constitutes the training set. We repetead this sampling 10  times with different seeds to cross validate our results. The average values (and standard deviations) computed on the ten sub-training sets are reported, for each proportion, as final results.

% stopping citerion
For deciding when to stop the inference process, 10\% of the training set used serves as a validation set on which the log-likelihood is computed after each minibatch iteration. When the increase of the log-likelihood, averaged over the last 20 measures, is less than 0.001, the inference is stopped. The log-likelihood of a given set of observations $\D_{set}$  is given by:
\begin{equation*}
\log p(\D_{set}) = \sum_{i,j \in \D_{set}} \log p(y_{ij} | \phih_{kk'}) p(k|\thetah_i) p(k'|\thetah_j)
%\log p(\D_{test}) = \sum_{i,j \in \D_{test}} \log p(y_{ij} | \phih_{kk'}) p(k|\thetah_i) p(k'|\thetah_j)
\end{equation*}

For all our models, the gradient step parameters  $\tau$ and $\kappa$ were fixed respectively to  $1024$ and $0.5$, the burn-in period $T_{burnin}$ to $150$; for stratified sampling, $M$ was set to $50$, the size of $s_0^{i,m}, \, 1 \le m \le M$ being equal to the number of nodes to which $i$ is not connected to divided by $M$. For MMSB, the hyperparameters $\lambda_0$ and $\lambda_1$ were set to $0.1$. For WMMSB, the shape and scale parameters $r$ et $p$ were fixed to $1$ and for WMMSB, the beta-gamma hyperparameters were fixed to  $c_0=10$, $r_0=1$, $c=100$ and $\epsilon=10^{-6}$. The number of latent classes $K$ was fixed to $10$ for all models and the latent-class hyperparameters $\alpha_k$ to $\frac{1}{K}$. Our implementation is available online\footnote{https://github.com/***/*** (anonymized)}. 


\begin{table*}[t]
\centering
	\input{fig2/wsim_training100}
\label{table:mse}
\end{table*}

\subsection{Results}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Prediction performance
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Table \ref{table:mse}, gives the MSE score for the different models and for all networks. To evaluate the ability of the models to reconstruct the missing weights, we compute the MSE only on the missing edges of the test data. Although th models are also able to predict the presence or absence of edges,  we considered that the question of recovering the edges structure should be addressed separately by other link prediction method. 
As one can note, WMMSB-gt outperforms other models except for the network moreno\_names. 
The fact that this network is the only one in the category of linguistic network and that it is relatively small could explain this result. EPM outperforms other models, apart from WMMSB-gt. Other models expose comparable results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Sensibility on K 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 

\begin{figure}[h]
\centering
	\input{fig2/wsim_k_evo}
   \label{fig:k_evolv}
\end{figure}

Figure \ref{fig:k_evolv} shows the MSE score for several number of latent classes. One can see that the result are stable with the number of classes between 10 and 50.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Timing performance
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
	\input{fig2/timing_100_k10.tex}
\label{table:time}
\end{table*}

Table \ref{table:time} shows the inference time of the models on the different dataset in hours. The models *MMSB, SBM-ai-* and *SBM-gt are implemented respectively in Python, Matlab and C++ (with a Python wrapper \cite{peixoto_graph-tool_2014}), which must be considered when comparing the models, Furthermore, and hard limit to 25 hours was set for the inference time to limit the computing cost. That being said, the *SBM-gt is the fastest model while EPM is the slowest and can take more than 20 times compared to *MMSB inference. Most, notably, the inference of *MMSB with SCVB is fit millions of edge in less than 2 hours, while implemented in python, which is encouraging in the development of this inference scheme.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Convergence of *MMSB SCVB inference
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Figure \ref{fig:conv_entropy} shows the evolution of the log-likelihood for the MMSB-based models on the test set. We used three different sets for the hyperparameters shape $r$ and scale $p$ of WMMSB. Regardless of the values of these hyperparameters, one can observe that the augmented model WMMSB-bg is less prone to overfitting, usually converges to a better solution and only needs a small proportion of the total number $N^2$ of edges to do so.

\begin{figure}[h]
\centering
	\input{img/conv_entropy3}
    \label{fig:conv_entropy}
\end{figure}







