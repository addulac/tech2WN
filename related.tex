\section{Related work}
\label{sec:rl}

%\begin{itemize}
%\item on SBM and WSBM (Clauset/peixoto)
%\item on MMSB familly (Airoldi/Blei/Mimmo/Gopalan) and SVB
%\item on PFA (Poisson Factor Analysis) and Gamma Processes (Zhou etc).
%\item on SCVB (Foulds). (they show that scvb is similar to EM+map on made the links with online EM of (Capp√© and Moulines)
%\end{itemize}

The original MMSB model was proposed in \cite{airoldi2009mixed} with a variational inference scheme. The inference process was later extended with stochastic variational inference in \cite{gopalan2013efficient} and structured variational inference in \cite{kim2013efficient} for scalability purposes. Stochastic variational inference has been applied with a collapsed variational objective for the latent Dirichlet allocation model \cite{foulds2013stochastic}. To our knowledge, it is the first time that stochastic and collapsed variational inference are coupled in the context of stochastic block models.

A weighted version of the stochastic block model has been proposed in \cite{aicher2014learning}; it can be seen as a special case of the WMMSB model proposed in this paper in which nodes are constrained to belong to only one latent class. More recently, another type of weighted stochastic block models has been proposed \cite{peixoto2018nonparametric} in which different kernels can be used to model different types of weights. An efficient MCMC method is used for inference. If this type of models is interesting, it nevertheless relies again on the unreasonable assumption that a node belongs to only one class. Furthermore, unlike MMSB models, the lack of a hierarchical prior structure does not allow one to rely on efficient non-parametric extensions (hence the use of costly model selection techniques for non-parametric versions).

Similar to our model, count processes with Poisson distributions and Gamma conjugate priors have been studied by different authors \cite{zhou2012augment, zhou2015negative}. The relation of such processes with Negative Binomial processes is well-known and is highlighted by these authors. Such processes can be used for topic modeling, as the Beta-Gamma-Gamma-Poisson model of \cite{zhou2012beta} that relies on MCMC inference. The main difference between this model and WMMSB is that the former factorizes counts as Poisson variables of a sum of latent factors while, in WMMSB, counts are factorized as a convex sum of Poisson variable depending on class memberships.

%
%This work intersects with several groups of related works:
%
%First, the recent advance on Stochatistic Variationnal inference have made it possible to scale bayesian model to bigger dataset and to do online learning which enable a low memory footprint. This inference have first been proposed for topic modeling [1][2] before being adapted for the MMSB model with an adaptation to discover overllaping communities [3] [4],
%
%Nevertheless, the previous works only study the case of (undirected) binary networks.
%
%In [5] the author proposed an efficient inference algorithm for weigthed networks, based on a MCMC algorithms. The model is an extension of the SBM. Those models assumed that the class don't overllap. (I still have to dive into to understand how his inferecne works...) (does it allow online learning ? )
%
%Finally, SVB has been combined with CVB inference for topic modelling to propose a improoved over SVB. [6]
%
%This paper combines the different advantage of those works to propose a Online learning algorithm to models networks that can be weighted, with overllaping classes, directed or undirected.
%

